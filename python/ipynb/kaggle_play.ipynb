{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import threading\n",
    "import time\n",
    "import concurrent.futures\n",
    "import random\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## producer - pipeline - consumer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Threads wrap functions. thread.start causes a thread to start executing a function, whilst join blocks until the thread is done executing.\n",
    "* Python's default programming model has the main thread wait for all spawned threads to finish executing before exiting. It is possible to not wait on a thread to finish by setting the daemon flag instead.\n",
    "* The simplest way to manage concurrency in threads is to start a list of threads and then join them sequentially. This is fine, but there is a built-in that does this for you: `concurrent.futures.ThreadPoolExecutor`. That looks thus:\n",
    "\n",
    "> `with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "      executor.map(thread_function, range(3))\n",
    "`\n",
    "* There is some confusing aspects of how this API works and is run. In particular, errors in the function signature of thread_function will be swallowed without an error being raised.\n",
    "\n",
    "* The `GIL` held by a thread will always be released during I/O operations, time.sleep, and certain computationally intensive operations in e.g. numpy (there might be other operations but these are the two that I'm aware of). These are scenarios in which the thread gives the GIL up voluntarily. In other cases, the thread manager will release the GIL, and a new thread will apply it; these are free to occur at any time step in the code. so these are the occassions during which race conditions are possible.\n",
    "* Data is __thread-safe__ if:\n",
    "n It is data local to the thread, e.g. it is not shared memory.\n",
    "> * It is shared memory but the data structure itself is thread-safe. For example, a queue, or an append-only data structure with no total ordering.\n",
    "> * To avoid race conditions during non thread-safe segments, take a lock with `threading.Lock()`. You can acquire and release this lock using a `context manager` or using `l.acquire()` and then `l.release()`. It has a `blocking=True` argument and a `timeout`. Using a non-blocking lock will cause the lock to return False if the lock cannot be acquired (because it is held by another thread), allowing you to do the required logic to avoid deadlocks yourself.\n",
    "* That gets us to the next, more complex example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:48:39: Producer got message: 68\n",
      "12:48:39: Producer got message: 84\n",
      "12:48:39: Consumer storing message: 68\n",
      "12:48:39: Producer got message: 72\n",
      "12:48:39: Consumer storing message: 84\n",
      "12:48:39: Producer got message: 59\n",
      "12:48:39: Consumer storing message: 72\n",
      "12:48:39: Producer got message: 14\n",
      "12:48:39: Consumer storing message: 59\n",
      "12:48:39: Producer got message: 48\n",
      "12:48:39: Consumer storing message: 14\n",
      "12:48:39: Producer got message: 41\n",
      "12:48:39: Consumer storing message: 48\n",
      "12:48:39: Producer got message: 24\n",
      "12:48:39: Consumer storing message: 41\n",
      "12:48:39: Producer got message: 19\n",
      "12:48:39: Consumer storing message: 24\n",
      "12:48:39: Producer got message: 82\n",
      "12:48:39: Consumer storing message: 19\n",
      "12:48:39: Consumer storing message: 82\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import concurrent.futures\n",
    "\n",
    "SENTINEL = object()\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"\n",
    "    Class to allow a single element pipeline between producer and consumer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.message = 0\n",
    "        self.producer_lock = threading.Lock()\n",
    "        self.consumer_lock = threading.Lock()\n",
    "        self.consumer_lock.acquire()\n",
    "\n",
    "    def get_message(self, who):\n",
    "        logging.debug(\"%s:about to acquire getlock\", who)\n",
    "        self.consumer_lock.acquire()\n",
    "        logging.debug(\"%s:have getlock\", who)\n",
    "        message = self.message\n",
    "        logging.debug(\"%s:about to release setlock\", who)\n",
    "        self.producer_lock.release()\n",
    "        logging.debug(\"%s:setlock released\", who)\n",
    "        return message\n",
    "\n",
    "    def set_message(self, message, who):\n",
    "        logging.debug(\"%s:about to acquire setlock\", who)\n",
    "        self.producer_lock.acquire()\n",
    "        logging.debug(\"%s:have setlock\", who)\n",
    "        self.message = message\n",
    "        logging.debug(\"%s:about to release getlock\", who)\n",
    "        self.consumer_lock.release()\n",
    "        logging.debug(\"%s:getlock released\", who)\n",
    "\n",
    "\n",
    "def producer(pipeline):\n",
    "    \"\"\"Pretend we're getting a message from the network.\"\"\"\n",
    "    for index in range(10):\n",
    "        message = random.randint(1, 101)\n",
    "        logging.info(\"Producer got message: %s\", message)\n",
    "        pipeline.set_message(message, \"Producer\")\n",
    "\n",
    "    # Send a sentinel message to tell consumer we're done\n",
    "    pipeline.set_message(SENTINEL, \"Producer\")\n",
    "\n",
    "\n",
    "def consumer(pipeline):\n",
    "    \"\"\"Pretend we're saving a number in the database.\"\"\"\n",
    "    message = 0\n",
    "    while message is not SENTINEL:\n",
    "        message = pipeline.get_message(\"Consumer\")\n",
    "        if message is not SENTINEL:\n",
    "            logging.info(\"Consumer storing message: %s\", message)\n",
    "\n",
    "\n",
    "format = \"%(asctime)s: %(message)s\"\n",
    "logging.basicConfig(format=format, level=logging.INFO,\n",
    "                    datefmt=\"%H:%M:%S\")\n",
    "\n",
    "pipeline = Pipeline()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    executor.submit(producer, pipeline)\n",
    "    executor.submit(consumer, pipeline)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## producer - consumer by Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import queue # FIFO\n",
    "\n",
    "def producer(queue, event):\n",
    "    \"\"\"Pretend we're getting a number from the network.\"\"\"\n",
    "    while not event.is_set():\n",
    "        message = random.randint(1, 101)\n",
    "        logging.info(\"Producer got message: %s\", message)\n",
    "        queue.put(message)\n",
    "\n",
    "    logging.info(\"Producer received event. Exiting\")\n",
    "\n",
    "\n",
    "def consumer(queue, event):\n",
    "    \"\"\"Pretend we're saving a number in the database.\"\"\"\n",
    "    while not event.is_set() or not queue.empty():\n",
    "        message = queue.get()\n",
    "        logging.info(\n",
    "            f\"Consumer storing message: {message} (size={queue.qsize()})\"\n",
    "        )\n",
    "\n",
    "    logging.info(\"Consumer received event. Exiting\")\n",
    "\n",
    "\n",
    "format = \"%(asctime)s: %(message)s\"\n",
    "logging.basicConfig(format=format, level=logging.INFO,\n",
    "                    datefmt=\"%H:%M:%S\")\n",
    "\n",
    "pipeline = queue.Queue(maxsize=10)\n",
    "event = threading.Event()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    executor.submit(producer, pipeline, event)\n",
    "    executor.submit(consumer, pipeline, event)\n",
    "\n",
    "    time.sleep(0.1)\n",
    "    logging.info(\"Main: about to set event\")\n",
    "    event.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<queue.Queue object at 0x0000020CF34BC6D0> 0\n"
     ]
    }
   ],
   "source": [
    "print(pipeline, pipeline.qsize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
